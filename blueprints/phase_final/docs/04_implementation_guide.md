# å®æ–½æŒ‡å— - Vibe Photos Phase Final

## ğŸš€ å¿«é€Ÿå¼€å§‹è·¯çº¿å›¾

### Week 0: å‡†å¤‡é˜¶æ®µï¼ˆ1-2å¤©ï¼‰
```bash
# 1. åˆ›å»ºæ–°é¡¹ç›®
mkdir vibe-photos-phase_final
cd vibe-photos-phase_final

# 2. åˆå§‹åŒ–ç¯å¢ƒ
uv init
uv add torch==2.9.0 transformers==4.57.1 pillow==11.3.0 fastapi==0.121.1 typer rich==14.2.0

# 3. éªŒè¯ç¯å¢ƒ
uv run python -c "import torch; print(torch.__version__)"
```

### Week 1: MVPå®ç°ï¼ˆ5å¤©ï¼‰

#### Day 1-2: æ ¸å¿ƒæ£€æµ‹å™¨
```python
# src/detector.py - åŸºç¡€SigLIPæ£€æµ‹å™¨ï¼ˆMVPé˜¶æ®µï¼‰
from transformers import AutoModel, AutoProcessor

class SimpleDetector:
    def __init__(self):
        self.model = AutoModel.from_pretrained("google/siglip-base-patch16-224-i18n")
        self.processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224-i18n")
    
    def classify(self, image_path, categories):
        # å®ç°åŸºç¡€åˆ†ç±»
        pass

# src/siglip_blip_detector.py - å¤šè¯­è¨€å›¾åƒç†è§£ï¼ˆPhase 1/2ï¼‰
# å®‰è£…: pip install transformers torch pillow
from transformers import AutoProcessor, AutoModel, BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

class SigLIPBLIPDetector:
    def __init__(self):
        # ä½¿ç”¨SigLIPè¿›è¡Œå¤šè¯­è¨€åˆ†ç±»
        self.siglip_processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224-i18n")
        self.siglip_model = AutoModel.from_pretrained("google/siglip-base-patch16-224-i18n")
        
        # ä½¿ç”¨BLIPç”Ÿæˆå›¾åƒæè¿°
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    
    def detect(self, image_path, candidate_labels=None):
        image = Image.open(image_path)
        
        # é›¶æ ·æœ¬åˆ†ç±»ï¼ˆæ”¯æŒä¸­æ–‡æ ‡ç­¾ï¼‰
        if candidate_labels is None:
            candidate_labels = ["æ‰‹æœº", "iPhone", "ç”µè„‘", "ç¾é£Ÿ", "æ–‡æ¡£"]
        
        # SigLIPåˆ†ç±»
        inputs = self.siglip_processor(text=candidate_labels, images=image, 
                                      padding=True, return_tensors="pt")
        outputs = self.siglip_model(**inputs)
        logits = outputs.logits_per_image
        probs = torch.sigmoid(logits)  # SigLIPä½¿ç”¨sigmoid
        
        # BLIPæè¿°
        caption_inputs = self.blip_processor(image, return_tensors="pt")
        caption = self.blip_model.generate(**caption_inputs)
        caption_text = self.blip_processor.decode(caption[0], skip_special_tokens=True)
        
        return {
            'classifications': dict(zip(candidate_labels, probs[0].tolist())),
            'caption': caption_text
        }
```

#### Day 3: æ•°æ®å±‚
```python
# src/database.py
import sqlite3
from datetime import datetime

def init_database():
    conn = sqlite3.connect('vibe_photos.db')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS photos (
            id INTEGER PRIMARY KEY,
            path TEXT UNIQUE,
            category TEXT,
            confidence REAL,
            created_at TIMESTAMP
        )
    ''')
```

#### Day 4: CLIå·¥å…·
```python
# src/cli.py
import typer

app = typer.Typer()

@app.command()
def import_photos(path: Path):
    """å¯¼å…¥ç…§ç‰‡"""
    pass

@app.command()
def search(query: str):
    """æœç´¢ç…§ç‰‡"""
    pass
```

#### Day 5: æµ‹è¯•å’Œä¼˜åŒ–
```python
# tests/test_detector.py
def test_basic_classification():
    detector = SimpleDetector()
    result = detector.classify("test.jpg", ["ç”µå­äº§å“", "ç¾é£Ÿ"])
    assert result.category in ["ç”µå­äº§å“", "ç¾é£Ÿ"]
```

### Week 2: æ ¸å¿ƒåŠŸèƒ½ï¼ˆ5å¤©ï¼‰

#### åŠŸèƒ½æ¸…å•
- [ ] OCRé›†æˆï¼ˆPaddleOCRï¼‰
- [ ] å“ç‰Œè¯†åˆ«ï¼ˆæ‰©å±•SigLIPï¼‰
- [ ] Web UIï¼ˆGradioï¼‰
- [ ] æ‰¹é‡å¤„ç†ä¼˜åŒ–
- [ ] æ ‡æ³¨åŠ©æ‰‹

### Month 1: å®Œæ•´ç³»ç»Ÿ

#### é‡Œç¨‹ç¢‘
- [ ] Few-shotå­¦ä¹ 
- [ ] é«˜çº§æœç´¢
- [ ] Reactå‰ç«¯
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] éƒ¨ç½²è„šæœ¬

## ğŸ— é¡¹ç›®ç»“æ„

### æ¨èç›®å½•ç»“æ„
```
vibe-photos-phase_final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ detector.py       # AIæ£€æµ‹
â”‚   â”‚   â”œâ”€â”€ recognizer.py     # æ··åˆè¯†åˆ«
â”‚   â”‚   â”œâ”€â”€ learner.py        # Few-shotå­¦ä¹ 
â”‚   â”‚   â””â”€â”€ ocr.py            # æ–‡å­—æå–
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ database.py       # æ•°æ®åº“æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ models.py         # æ•°æ®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ cache.py          # ç¼“å­˜ç®¡ç†
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py            # FastAPIä¸»åº”ç”¨
â”‚   â”‚   â”œâ”€â”€ routes.py         # APIè·¯ç”±
â”‚   â”‚   â””â”€â”€ schemas.py        # Pydanticæ¨¡å‹
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ gradio_app.py     # Gradioç•Œé¢
â”‚   â”‚   â””â”€â”€ static/           # é™æ€èµ„æº
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ main.py           # CLIå…¥å£
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_detector.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml
â”‚   â””â”€â”€ categories.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.py              # åˆå§‹åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ import_photos.py      # æ‰¹é‡å¯¼å…¥
â”‚   â””â”€â”€ benchmark.py          # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ API.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

## ğŸ“ å¼€å‘è§„èŒƒ

### ä»£ç é£æ ¼
```python
# 1. ä½¿ç”¨ç±»å‹æç¤º
from typing import List, Dict, Optional
from pathlib import Path

def process_image(
    image_path: Path,
    categories: List[str],
    confidence_threshold: float = 0.5
) -> Dict[str, float]:
    """å¤„ç†å•å¼ å›¾ç‰‡"""
    pass

# 2. ä½¿ç”¨Pydanticæ¨¡å‹
from pydantic import BaseModel

class PhotoMetadata(BaseModel):
    path: str
    category: str
    confidence: float
    tags: List[str] = []

# 3. å¼‚æ­¥ä¼˜å…ˆ
async def batch_process(images: List[Path]):
    tasks = [process_image(img) for img in images]
    return await asyncio.gather(*tasks)
```

### é”™è¯¯å¤„ç†
```python
# ä¼˜é›…çš„é”™è¯¯å¤„ç†
class DetectorError(Exception):
    """æ£€æµ‹å™¨åŸºç¡€å¼‚å¸¸"""
    pass

class ModelNotFoundError(DetectorError):
    """æ¨¡å‹æœªæ‰¾åˆ°"""
    pass

def safe_detect(image_path: Path):
    try:
        return detector.detect(image_path)
    except FileNotFoundError:
        logger.error(f"Image not found: {image_path}")
        return None
    except DetectorError as e:
        logger.error(f"Detection failed: {e}")
        return {"error": str(e)}
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### æµ‹è¯•é‡‘å­—å¡”
```
        /\
       /UI\       (10%) - E2Eæµ‹è¯•
      /----\
     /  API \     (20%) - é›†æˆæµ‹è¯•
    /--------\
   /  Unit    \   (70%) - å•å…ƒæµ‹è¯•
  /____________\
```

### æµ‹è¯•ç¤ºä¾‹
```python
# å•å…ƒæµ‹è¯•
def test_image_classification():
    detector = SimpleDetector()
    result = detector.classify("fixtures/iphone.jpg", ["æ‰‹æœº", "ç”µè„‘"])
    assert result["æ‰‹æœº"] > result["ç”µè„‘"]

# é›†æˆæµ‹è¯•
@pytest.mark.asyncio
async def test_api_search():
    async with AsyncClient(app=app) as client:
        response = await client.get("/search?q=iPhone")
        assert response.status_code == 200
        assert len(response.json()["results"]) > 0

# æ€§èƒ½æµ‹è¯•
def test_batch_performance():
    images = list(Path("test_images").glob("*.jpg"))
    start = time.time()
    results = batch_process(images)
    elapsed = time.time() - start
    assert elapsed < len(images) * 0.5  # < 0.5ç§’/å¼ 
```

## ğŸš¦ CI/CDé…ç½®

### GitHub Actions
```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install uv
        uv sync
    
    - name: Run tests
      run: |
        uv run pytest tests/
    
    - name: Lint
      run: |
        uv run ruff check src/
```

## ğŸš€ SigLIP+BLIPéƒ¨ç½²æŒ‡å—

### å®‰è£…ä¾èµ–
```bash
# 1. å®‰è£…æ ¸å¿ƒä¾èµ–
uv add torch torchvision
uv add transformers pillow

# 2. æ¨¡å‹ä¸‹è½½ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½ï¼‰
# SigLIP: google/siglip-base-patch16-224-i18n (~400MB)
# BLIP: Salesforce/blip-image-captioning-base (~990MB)
```

### SigLIP+BLIP vs RTMDetå¯¹æ¯”
| ç‰¹æ€§ | SigLIP+BLIP | RTMDet | ä¼˜åŠ¿ |
|------|-------------|---------|------|
| **ä¾èµ–** | transformersâœ… | mmcvâŒ | æ— å®‰è£…é—®é¢˜ |
| **å¤šè¯­è¨€** | æ”¯æŒâœ… | ä¸æ”¯æŒâŒ | ä¸­è‹±æ—¥ç­‰ |
| **é›¶æ ·æœ¬** | æ”¯æŒâœ… | ä¸æ”¯æŒâŒ | æ— éœ€é¢„å®šä¹‰ç±»åˆ« |
| **æè¿°ç”Ÿæˆ** | æ”¯æŒâœ… | ä¸æ”¯æŒâŒ | è‡ªç„¶è¯­è¨€æè¿° |
| **Python 3.11+** | æ”¯æŒâœ… | ä¸æ”¯æŒâŒ | ç°ä»£Pythonç‰ˆæœ¬ |

### ä½¿ç”¨ç¤ºä¾‹
```python
# å¤šè¯­è¨€å›¾åƒç†è§£
from src.siglip_blip_detector import SigLIPBLIPDetector

detector = SigLIPBLIPDetector()

# æ”¯æŒä¸­æ–‡æ ‡ç­¾
results = detector.detect(
    "product_photo.jpg",
    candidate_labels=["æ‰‹æœº", "iPhone", "ç”µè„‘", "MacBook", "ç¾é£Ÿ", "æŠ«è¨"]
)

# è‡ªåª’ä½“å†…å®¹åˆ†æ
for obj in results:
    if obj['score'] > 0.7:  # é«˜ç½®ä¿¡åº¦ç‰©ä½“
        print(f"æ£€æµ‹åˆ°: {obj['label']} - {obj['score']:.1%}")
        # è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾: #ç”µå­äº§å“ #iPhoneç­‰
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—é…ç½®
```python
# src/utils/logging.py
import logging
from rich.logging import RichHandler

def setup_logging(level="INFO"):
    logging.basicConfig(
        level=level,
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True)]
    )
    return logging.getLogger("vibe_photos")

# ä½¿ç”¨
logger = setup_logging()
logger.info("Processing image", extra={"path": "IMG_001.jpg"})
```

### æ€§èƒ½ç›‘æ§
```python
# src/utils/metrics.py
import time
from contextlib import contextmanager

@contextmanager
def timer(name: str):
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    logger.info(f"{name} took {elapsed:.2f} seconds")

# ä½¿ç”¨
with timer("batch_import"):
    import_photos(photo_dir)
```

## ğŸ¯ å…³é”®å®æ–½è¦ç‚¹

### 1. æ¸è¿›å¼å¼€å‘
```python
# Version 1: ç®€å•å®ç°
def search_v1(query):
    return db.execute("SELECT * FROM photos WHERE category LIKE ?", f"%{query}%")

# Version 2: æ·»åŠ ç›¸ä¼¼åº¦
def search_v2(query):
    results = search_v1(query)
    return sorted(results, key=lambda x: similarity(query, x.tags))

# Version 3: å‘é‡æœç´¢
def search_phase_final(query):
    embedding = encode_query(query)
    return vector_db.search(embedding, top_k=20)
```

### 2. åŠŸèƒ½å¼€å…³
```python
# config/features.yaml
features:
  ocr_enabled: false  # é€æ­¥å¯ç”¨
  brand_detection: false
  few_shot_learning: false

# ä»£ç ä¸­
if config.features.ocr_enabled:
    text = extract_text(image)
```

### 3. æ€§èƒ½ä¼˜åŒ–æ¸…å•
- [ ] ä½¿ç”¨ç¼©ç•¥å›¾è¿›è¡Œåˆæ­¥åˆ†ç±»
- [ ] æ‰¹é‡å¤„ç†å›¾ç‰‡
- [ ] ç¼“å­˜æ¨¡å‹é¢„æµ‹ç»“æœ
- [ ] å¼‚æ­¥I/Oæ“ä½œ
- [ ] è¿æ¥æ± ç®¡ç†

## ğŸš€ éƒ¨ç½²æŒ‡å—

### å¼€å‘ç¯å¢ƒ
```bash
# 1. å…‹éš†ä»£ç 
git clone <repo>
cd vibe-photos-phase_final

# 2. å®‰è£…ä¾èµ–
uv sync

# 3. è¿è¡Œå¼€å‘æœåŠ¡å™¨
uv run uvicorn src.api.app:app --reload
```

### ç”Ÿäº§éƒ¨ç½²
```bash
# 1. æ„å»ºDockeré•œåƒ
docker build -t vibe-photos:phase_final .

# 2. è¿è¡Œå®¹å™¨
docker run -d \
  -p 8000:8000 \
  -v /path/to/photos:/photos \
  -v /path/to/data:/data \
  vibe-photos:phase_final

# 3. ä½¿ç”¨systemdï¼ˆå¯é€‰ï¼‰
sudo cp vibe-photos.service /etc/systemd/system/
sudo systemctl enable vibe-photos
sudo systemctl start vibe-photos
```

## ğŸ“‹ å®æ–½æ£€æŸ¥æ¸…å•

### Week 1 äº¤ä»˜
- [ ] åŸºç¡€åˆ†ç±»å·¥ä½œ
- [ ] æ•°æ®åº“åˆ›å»ºå’Œè¿æ¥
- [ ] CLIå¯ä»¥å¯¼å…¥ç…§ç‰‡
- [ ] ç®€å•æœç´¢åŠŸèƒ½
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡

### Week 2 äº¤ä»˜
- [ ] OCRåŠŸèƒ½é›†æˆ
- [ ] Webç•Œé¢å¯è®¿é—®
- [ ] æ‰¹é‡å¤„ç†ä¼˜åŒ–
- [ ] APIæ–‡æ¡£å®Œæˆ
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### Month 1 äº¤ä»˜
- [ ] å®Œæ•´åŠŸèƒ½å®ç°
- [ ] å‰ç«¯ç•Œé¢ç¾è§‚
- [ ] éƒ¨ç½²è„šæœ¬å°±ç»ª
- [ ] ç”¨æˆ·æ–‡æ¡£å®Œæ•´
- [ ] æ€§èƒ½è¾¾æ ‡

## ğŸ’¡ å®æ–½å»ºè®®

1. **å…ˆè·‘é€šæµç¨‹**ï¼Œå†ä¼˜åŒ–æ€§èƒ½
2. **å…ˆæœ¬åœ°éƒ¨ç½²**ï¼Œå†è€ƒè™‘äº‘ç«¯
3. **å…ˆå•ç”¨æˆ·**ï¼Œå†å¤šç”¨æˆ·
4. **å…ˆè‹±æ–‡**ï¼Œå†å¤šè¯­è¨€
5. **å…ˆCPU**ï¼Œå†GPUåŠ é€Ÿ

## ğŸ å¿«é€Ÿå¯åŠ¨æ¨¡æ¿

```python
# quickstart.py
"""
Vibe Photos Phase Final - å¿«é€Ÿå¯åŠ¨æ¨¡æ¿
ç›´æ¥è¿è¡Œçœ‹æ•ˆæœï¼šuv run quickstart.py /path/to/photos
"""

import typer
from pathlib import Path
from transformers import pipeline

def main(photo_dir: Path):
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = pipeline("zero-shot-image-classification")
    
    # å¤„ç†ç…§ç‰‡
    for image in photo_dir.glob("*.jpg"):
        result = classifier(
            str(image),
            candidate_labels=["ç”µå­äº§å“", "ç¾é£Ÿ", "æ–‡æ¡£", "é£æ™¯"]
        )
        print(f"{image.name}: {result[0]['label']} ({result[0]['score']:.1%})")

if __name__ == "__main__":
    typer.run(main)
```

---

å‡†å¤‡å¥½å¼€å§‹äº†å—ï¼Ÿè®©æˆ‘ä»¬åœ¨æ–°ä»“åº“ä¸­å®ç°è¿™ä¸ªè®¾è®¡ï¼ ğŸš€

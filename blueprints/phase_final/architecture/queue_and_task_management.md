# æ¶ˆæ¯é˜Ÿåˆ—ä¸ä»»åŠ¡ç¼–æ’æ–¹æ¡ˆ

## ğŸ“¬ æŠ€æœ¯é€‰å‹

### ä¸»é€‰æ–¹æ¡ˆï¼šCelery + Redis

#### é€‰æ‹©ç†ç”±
- **æˆç†Ÿç¨³å®š**ï¼šCeleryæ˜¯Pythonç”Ÿæ€æœ€æˆç†Ÿçš„ä»»åŠ¡é˜Ÿåˆ—æ–¹æ¡ˆ
- **æ˜“äºé›†æˆ**ï¼šä¸FastAPIæ— ç¼é›†æˆï¼ŒåŸç”ŸPythonæ”¯æŒ
- **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒå®šæ—¶ä»»åŠ¡ã€ä»»åŠ¡é“¾ã€é‡è¯•ã€ä¼˜å…ˆçº§ç­‰
- **ç›‘æ§å‹å¥½**ï¼šFloweræä¾›å¯è§†åŒ–ç›‘æ§ç•Œé¢
- **è½»é‡éƒ¨ç½²**ï¼šRedisä½œä¸ºBrokerï¼Œèµ„æºå ç”¨å°

#### æ¶æ„è®¾è®¡
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FastAPI App                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     API Endpoints (å¼‚æ­¥æ¥æ”¶)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼ æäº¤ä»»åŠ¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Redis (Message Broker)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Queue: high_priority (å®æ—¶å¤„ç†)    â”‚  â”‚
â”‚  â”‚   Queue: default (å¸¸è§„ä»»åŠ¡)          â”‚  â”‚
â”‚  â”‚   Queue: batch (æ‰¹é‡å¤„ç†)            â”‚  â”‚
â”‚  â”‚   Queue: learning (æ¨¡å‹è®­ç»ƒ)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼ æ¶ˆè´¹ä»»åŠ¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Celery Workers Pool               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker1: å›¾åƒæ£€æµ‹ (GPUä¼˜åŒ–)         â”‚  â”‚
â”‚  â”‚  Worker2: OCRå¤„ç† (CPUå¯†é›†)          â”‚  â”‚
â”‚  â”‚  Worker3: å‘é‡è®¡ç®— (å¹¶è¡Œå¤„ç†)        â”‚  â”‚
â”‚  â”‚  Worker4: æ•°æ®åŒæ­¥ (I/Oå¯†é›†)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ ä»»åŠ¡ç±»å‹å®šä¹‰

### 1. å®æ—¶ä»»åŠ¡ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
```python
# tasks/realtime.py
from celery import Task
from celery.exceptions import Retry

class ImageDetectionTask(Task):
    """å•å¼ å›¾ç‰‡å®æ—¶æ£€æµ‹"""
    name = 'detect.single'
    max_retries = 3
    default_retry_delay = 5
    
    def run(self, image_path: str, user_id: str):
        try:
            # SigLIP+BLIPæ£€æµ‹
            detections = siglip_blip_detector.detect(image_path)
            
            # SigLIPåˆ†ç±»
            category = siglip_classifier.classify(image_path)
            
            # ä¿å­˜ç»“æœ
            save_to_db(image_path, detections, category)
            
            # å®æ—¶æ¨é€ç»“æœ
            websocket_notify(user_id, {'status': 'completed', 'path': image_path})
            
        except Exception as e:
            # æŒ‡æ•°é€€é¿é‡è¯•
            raise self.retry(exc=e, countdown=2 ** self.request.retries)

@celery.task(bind=True, queue='high_priority', priority=9)
def detect_single_image(self, image_path: str, user_id: str):
    return ImageDetectionTask().run(image_path, user_id)
```

### 2. æ‰¹é‡å¤„ç†ä»»åŠ¡
```python
# tasks/batch.py
from celery import group, chain, chord

@celery.task(queue='batch')
def process_batch_import(folder_path: str, user_id: str):
    """æ‰¹é‡å¯¼å…¥ç…§ç‰‡"""
    
    # 1. æ‰«ææ–‡ä»¶
    images = scan_folder(folder_path)
    
    # 2. åˆ›å»ºä»»åŠ¡ç»„ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
    detection_group = group(
        detect_image.s(img) for img in images[:100]  # é™åˆ¶å¹¶å‘
    )
    
    # 3. ä¸²è¡Œä»»åŠ¡é“¾
    workflow = chain(
        validate_images.s(images),
        detection_group,
        aggregate_results.s(),
        update_vectors.s(),
        notify_completion.s(user_id)
    )
    
    return workflow.apply_async()

@celery.task(queue='batch')
def aggregate_results(results):
    """èšåˆæ‰¹å¤„ç†ç»“æœ"""
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    # æ›´æ–°ç»Ÿè®¡
    update_stats(len(successful), len(failed))
    
    # è§¦å‘å¢é‡å­¦ä¹ 
    if len(successful) > 50:
        trigger_incremental_learning.delay(successful)
    
    return {
        'total': len(results),
        'success': len(successful),
        'failed': len(failed)
    }
```

### 3. æ¨¡å‹è®­ç»ƒä»»åŠ¡
```python
# tasks/learning.py
@celery.task(queue='learning', time_limit=3600)  # 1å°æ—¶è¶…æ—¶
def train_few_shot_model(samples: List[dict], model_name: str):
    """Few-shotå­¦ä¹ ä»»åŠ¡"""
    
    # è·å–ç‹¬å é”ï¼Œé¿å…å¹¶å‘è®­ç»ƒ
    with redis_lock(f'training:{model_name}', timeout=3600):
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y = prepare_training_data(samples)
        
        # è®­ç»ƒæ¨¡å‹
        model = FewShotLearner()
        model.fit(X, y)
        
        # éªŒè¯æ€§èƒ½
        metrics = evaluate_model(model)
        
        if metrics['accuracy'] > 0.8:
            # ä¿å­˜æ¨¡å‹
            model_path = save_model(model, model_name)
            
            # ç‰ˆæœ¬ç®¡ç†
            register_model_version(model_name, model_path, metrics)
            
            # çƒ­æ›´æ–°æ¨¡å‹
            hot_reload_model.delay(model_name, model_path)
        
        return metrics
```

## ğŸ”„ ä»»åŠ¡è°ƒåº¦ç­–ç•¥

### 1. ä¼˜å…ˆçº§ç®¡ç†
```python
# config/celery_config.py
from kombu import Queue, Exchange

task_routes = {
    'detect.single': {'queue': 'high_priority', 'priority': 9},
    'batch.*': {'queue': 'batch', 'priority': 5},
    'learning.*': {'queue': 'learning', 'priority': 3},
    'sync.*': {'queue': 'default', 'priority': 1},
}

# é˜Ÿåˆ—å®šä¹‰
task_queues = [
    Queue('high_priority', Exchange('high_priority'), 
          routing_key='high', priority=10),
    Queue('batch', Exchange('batch'), 
          routing_key='batch', priority=5),
    Queue('learning', Exchange('learning'), 
          routing_key='learning', priority=3),
    Queue('default', Exchange('default'), 
          routing_key='default', priority=1),
]

# ä»»åŠ¡ç¡®è®¤æœºåˆ¶
task_acks_late = True  # ä»»åŠ¡å®Œæˆåæ‰ç¡®è®¤
task_reject_on_worker_lost = True  # Workerä¸¢å¤±æ—¶æ‹’ç»ä»»åŠ¡
```

### 2. é‡è¯•ç­–ç•¥
```python
# utils/retry_policy.py
from celery import Task
from celery.exceptions import MaxRetriesExceededError

class RetryableTask(Task):
    """å¯é‡è¯•ä»»åŠ¡åŸºç±»"""
    
    autoretry_for = (ConnectionError, TimeoutError)
    retry_kwargs = {'max_retries': 3}
    retry_backoff = True  # æŒ‡æ•°é€€é¿
    retry_backoff_max = 600  # æœ€å¤§é€€é¿æ—¶é—´10åˆ†é’Ÿ
    retry_jitter = True  # æ·»åŠ éšæœºæŠ–åŠ¨

@celery.task(base=RetryableTask)
def process_with_retry(image_path):
    """å¸¦é‡è¯•çš„å¤„ç†ä»»åŠ¡"""
    try:
        result = heavy_processing(image_path)
        return result
    except MaxRetriesExceededError:
        # é‡è¯•å¤±è´¥ï¼Œè¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
        send_to_dlq(image_path)
        notify_admin(f"Processing failed: {image_path}")
```

### 3. ä»»åŠ¡ç¼–æ’æ¨¡å¼
```python
# workflows/complex_workflow.py
from celery import chord, group, chain

def import_and_learn_workflow(folder_path: str):
    """å¤æ‚çš„å¯¼å…¥å’Œå­¦ä¹ å·¥ä½œæµ"""
    
    # é˜¶æ®µ1ï¼šå¹¶è¡Œæ£€æµ‹
    detection_tasks = group([
        detect_objects.s(img),
        extract_text.s(img),
        calculate_embedding.s(img)
    ] for img in get_images(folder_path))
    
    # é˜¶æ®µ2ï¼šèšåˆç»“æœ
    aggregate = aggregate_detection_results.s()
    
    # é˜¶æ®µ3ï¼šæ¡ä»¶åˆ†æ”¯
    def route_by_confidence(results):
        high_conf = [r for r in results if r['confidence'] > 0.8]
        low_conf = [r for r in results if r['confidence'] < 0.5]
        
        if high_conf:
            auto_label.delay(high_conf)
        if low_conf:
            queue_for_review.delay(low_conf)
    
    # ç»„åˆå·¥ä½œæµ
    workflow = chord(detection_tasks)(aggregate | route_by_confidence)
    
    return workflow
```

## ğŸ“Š ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 1. Flowerç›‘æ§é…ç½®
```python
# monitoring/flower_config.py
from flower import Flower

flower_config = {
    'broker': 'redis://localhost:6379/0',
    'port': 5555,
    'basic_auth': ['admin:secure_password'],
    'persistent': True,
    'db': 'flower.db',
    'max_tasks': 10000,
    'enable_events': True
}

# è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
custom_metrics = {
    'task_duration': histogram('task_duration_seconds'),
    'task_success_rate': gauge('task_success_rate'),
    'queue_length': gauge('queue_length'),
    'worker_utilization': gauge('worker_utilization')
}
```

### 2. æ€§èƒ½ç›‘æ§
```python
# monitoring/metrics.py
import time
from prometheus_client import Counter, Histogram, Gauge

# PrometheusæŒ‡æ ‡
task_counter = Counter('celery_tasks_total', 'Total tasks', ['task', 'status'])
task_duration = Histogram('celery_task_duration_seconds', 'Task duration', ['task'])
queue_size = Gauge('celery_queue_size', 'Queue size', ['queue'])

@celery.task(bind=True)
def monitored_task(self, *args, **kwargs):
    """å¸¦ç›‘æ§çš„ä»»åŠ¡è£…é¥°å™¨"""
    start_time = time.time()
    
    try:
        result = actual_task(*args, **kwargs)
        task_counter.labels(task=self.name, status='success').inc()
        return result
    except Exception as e:
        task_counter.labels(task=self.name, status='failure').inc()
        raise
    finally:
        duration = time.time() - start_time
        task_duration.labels(task=self.name).observe(duration)
```

## ğŸš¦ æµé‡æ§åˆ¶

### 1. é€Ÿç‡é™åˆ¶
```python
# config/rate_limits.py
rate_limits = {
    'detect.single': '100/m',  # æ¯åˆ†é’Ÿ100æ¬¡
    'batch.*': '10/m',  # æ¯åˆ†é’Ÿ10ä¸ªæ‰¹æ¬¡
    'learning.*': '1/h',  # æ¯å°æ—¶1æ¬¡è®­ç»ƒ
}

# åŠ¨æ€é€Ÿç‡è°ƒæ•´
@celery.task
def adjust_rate_limits():
    """æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´é€Ÿç‡"""
    cpu_usage = get_cpu_usage()
    memory_usage = get_memory_usage()
    
    if cpu_usage > 80 or memory_usage > 80:
        # é™ä½é€Ÿç‡
        celery.control.rate_limit('detect.single', '50/m')
    elif cpu_usage < 30 and memory_usage < 30:
        # æé«˜é€Ÿç‡
        celery.control.rate_limit('detect.single', '200/m')
```

### 2. èƒŒå‹å¤„ç†
```python
# utils/backpressure.py
from celery import signals

@signals.task_prerun.connect
def check_backpressure(sender=None, task_id=None, task=None, **kwargs):
    """ä»»åŠ¡æ‰§è¡Œå‰æ£€æŸ¥èƒŒå‹"""
    queue_size = get_queue_size(task.queue)
    
    if queue_size > 1000:
        # é˜Ÿåˆ—è¿‡é•¿ï¼Œæ‹’ç»æ–°ä»»åŠ¡
        raise QueueOverloadError(f"Queue {task.queue} is overloaded")
    
    if queue_size > 500:
        # è­¦å‘Šçº§åˆ«ï¼Œè®°å½•æ—¥å¿—
        logger.warning(f"Queue {task.queue} size: {queue_size}")
```

## ğŸ”§ éƒ¨ç½²é…ç½®

### 1. Docker Composeé…ç½®
```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  celery_worker:
    build: .
    command: celery -A app.celery worker -Q high_priority,default,batch -c 4
    depends_on:
      - redis
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    volumes:
      - ./photos:/photos
      - ./models:/models
    deploy:
      replicas: 2  # 2ä¸ªworkerå®ä¾‹

  celery_beat:
    build: .
    command: celery -A app.celery beat -l INFO
    depends_on:
      - redis
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0

  flower:
    build: .
    command: celery -A app.celery flower --port=5555
    ports:
      - "5555:5555"
    depends_on:
      - redis
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0

volumes:
  redis_data:
```

### 2. ç”Ÿäº§ç¯å¢ƒé…ç½®
```python
# config/production.py
CELERY_CONFIG = {
    'broker_url': 'redis://redis-cluster:6379/0',
    'result_backend': 'redis://redis-cluster:6379/1',
    
    # æŒä¹…åŒ–
    'task_serializer': 'json',
    'result_serializer': 'json',
    'accept_content': ['json'],
    
    # æ€§èƒ½ä¼˜åŒ–
    'worker_prefetch_multiplier': 4,
    'worker_max_tasks_per_child': 1000,
    'broker_pool_limit': 10,
    
    # å®¹é”™
    'task_acks_late': True,
    'task_reject_on_worker_lost': True,
    'task_publish_retry': True,
    'task_publish_retry_policy': {
        'max_retries': 3,
        'interval_start': 0,
        'interval_step': 0.2,
        'interval_max': 0.5,
    }
}
```

## ğŸ“‹ å®æ–½æ¸…å•

- [ ] å®‰è£…RedisæœåŠ¡å™¨
- [ ] é…ç½®CeleryåŸºç¡€æ¶æ„
- [ ] å®ç°ä»»åŠ¡ç±»å‹ï¼ˆå®æ—¶ã€æ‰¹é‡ã€å­¦ä¹ ï¼‰
- [ ] è®¾ç½®é˜Ÿåˆ—ä¼˜å…ˆçº§
- [ ] é…ç½®é‡è¯•ç­–ç•¥
- [ ] éƒ¨ç½²Flowerç›‘æ§
- [ ] å®ç°é€Ÿç‡é™åˆ¶
- [ ] æ·»åŠ PrometheusæŒ‡æ ‡
- [ ] é…ç½®æ—¥å¿—èšåˆ
- [ ] æµ‹è¯•å®¹é”™æœºåˆ¶
- [ ] ç¼–å†™ä»»åŠ¡ç¼–æ’ç¤ºä¾‹
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

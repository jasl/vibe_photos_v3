# å‘é‡æ•°æ®åº“ä¸æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ–¹æ¡ˆ

## ğŸ—‚ å‘é‡æ•°æ®åº“æ¶æ„

### æŠ€æœ¯é€‰å‹ï¼šPostgreSQL + pgvector

#### é€‰æ‹©ç†ç”±
- **ç»Ÿä¸€å­˜å‚¨**ï¼šå…ƒæ•°æ®ä¸å‘é‡åœ¨åŒä¸€æ•°æ®åº“ï¼Œé¿å…åŒæ­¥é—®é¢˜
- **äº‹åŠ¡ä¸€è‡´æ€§**ï¼šACIDä¿è¯ï¼Œæ•°æ®å®‰å…¨å¯é 
- **åŸç”ŸSQLæ”¯æŒ**ï¼šç»“åˆä¼ ç»ŸæŸ¥è¯¢ä¸å‘é‡æœç´¢
- **ç®€åŒ–è¿ç»´**ï¼šå•ä¸€ç³»ç»Ÿï¼Œé™ä½å¤æ‚åº¦
- **è¶³å¤Ÿæ€§èƒ½**ï¼šå¯¹äºç™¾ä¸‡çº§å‘é‡å®Œå…¨å¤Ÿç”¨ï¼ˆæˆ‘ä»¬åªæœ‰3ä¸‡å¼ ç…§ç‰‡ï¼‰

### ç³»ç»Ÿæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              åº”ç”¨å±‚                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚      Vector Service API          â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Write  â”‚ â”‚   Search    â”‚ â”‚   Update     â”‚
â”‚  Path   â”‚ â”‚   Path      â”‚ â”‚   Path       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚            â”‚               â”‚
    â–¼            â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PostgreSQL + pgvector (ä¸»å­˜å‚¨)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Metadata Tables                     â”‚  â”‚
â”‚  â”‚  - photos, detections, annotations   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Vector Storage (pgvector)           â”‚  â”‚
â”‚  â”‚  - HNSW index for fast search        â”‚  â”‚
â”‚  â”‚  - Cosine/L2 distance metrics        â”‚  â”‚
â”‚  â”‚  - Hybrid SQL+Vector queries         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
            [å¯é€‰æ‰©å±•ï¼šç™¾ä¸‡çº§+]
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Faiss Cache Layer (Optional)        â”‚
â”‚   ä»…å½“å‘é‡è¶…è¿‡100ä¸‡æ—¶è€ƒè™‘å¼•å…¥               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### pgvector æ€§èƒ½ç‰¹æ€§
```yaml
index_types:
  ivfflat:  # å€’æ’æ–‡ä»¶ç´¢å¼•
    - é€‚åˆ: 10ä¸‡-100ä¸‡å‘é‡
    - æ„å»ºå¿«ï¼ŒæŸ¥è¯¢å¿«
    - éœ€è¦å®šæœŸé‡å»º
    
  hnsw:  # åˆ†å±‚å¯¼èˆªå°ä¸–ç•Œå›¾
    - é€‚åˆ: ä»»æ„è§„æ¨¡ï¼ˆæ¨èï¼‰
    - æ„å»ºæ…¢ï¼ŒæŸ¥è¯¢æå¿«
    - å¢é‡å‹å¥½ï¼Œæ— éœ€é‡å»º
    - æˆ‘ä»¬çš„é€‰æ‹© âœ…

performance_benchmarks:
  30k_vectors:  # æˆ‘ä»¬çš„è§„æ¨¡
    - ç´¢å¼•æ„å»º: < 1åˆ†é’Ÿ
    - å•æ¬¡æŸ¥è¯¢: < 20ms
    - æ‰¹é‡æŸ¥è¯¢: < 100ms (50ä¸ª)
    - å†…å­˜å ç”¨: < 500MB
    
  1m_vectors:  # æœªæ¥æ‰©å±•
    - ç´¢å¼•æ„å»º: < 30åˆ†é’Ÿ
    - å•æ¬¡æŸ¥è¯¢: < 100ms
    - æ‰¹é‡æŸ¥è¯¢: < 500ms
    - å†…å­˜å ç”¨: < 8GB
```

## ğŸ“Š æ•°æ®å±‚è®¾è®¡

### æ•°æ®åº“Schemaï¼ˆä½¿ç”¨pgvectorï¼‰
```sql
-- å¯ç”¨pgvectoræ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- ç…§ç‰‡å‘é‡è¡¨ï¼ˆä¸»è¡¨ï¼‰
CREATE TABLE photo_embeddings (
    id BIGSERIAL PRIMARY KEY,
    photo_id BIGINT REFERENCES photos(id) ON DELETE CASCADE,
    
    -- å‘é‡ä¿¡æ¯
    embedding_model TEXT NOT NULL,  -- 'clip-vit-base', 'dinov2', etc
    embedding_version TEXT NOT NULL,  -- 'v1.0.0', 'v1.1.0'
    embedding_dimension INT NOT NULL,  -- 512, 768, 1024
    
    -- å‘é‡å­˜å‚¨ï¼ˆpgvectoråŸç”Ÿç±»å‹ï¼‰
    embedding vector(768) NOT NULL,  -- å¯è°ƒæ•´ç»´åº¦
    
    -- å…ƒæ•°æ®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,  -- è½¯åˆ é™¤æ ‡è®°
    
    -- çº¦æŸ
    UNIQUE(photo_id, embedding_model, embedding_version),
    CHECK (embedding_dimension > 0 AND embedding_dimension <= 4096)
);

-- åˆ›å»ºHNSWç´¢å¼•ï¼ˆæ¨èç”¨äºæˆ‘ä»¬çš„è§„æ¨¡ï¼‰
CREATE INDEX photo_embeddings_hnsw_idx ON photo_embeddings 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- æˆ–ä½¿ç”¨IVFFlatç´¢å¼•ï¼ˆæ›´å¿«æ„å»ºï¼Œé€‚åˆé¢‘ç¹æ›´æ–°ï¼‰
-- CREATE INDEX photo_embeddings_ivf_idx ON photo_embeddings 
-- USING ivfflat (embedding vector_l2_ops)
-- WITH (lists = 100);

-- å‘é‡ç´¢å¼•ç‰ˆæœ¬ç®¡ç†
CREATE TABLE vector_indices (
    id SERIAL PRIMARY KEY,
    index_name TEXT UNIQUE NOT NULL,
    index_type TEXT NOT NULL,  -- 'IVF', 'HNSW', 'Flat'
    
    -- ç´¢å¼•é…ç½®
    config JSONB NOT NULL,  -- {"nlist": 4096, "nprobe": 64}
    total_vectors INT DEFAULT 0,
    dimension INT NOT NULL,
    
    -- æ¨¡å‹ä¿¡æ¯
    base_model TEXT NOT NULL,
    model_version TEXT NOT NULL,
    
    -- æ–‡ä»¶è·¯å¾„
    index_path TEXT NOT NULL,
    backup_path TEXT,
    
    -- çŠ¶æ€ç®¡ç†
    status TEXT DEFAULT 'building',  -- building, active, deprecated
    is_primary BOOLEAN DEFAULT FALSE,
    
    -- æ€§èƒ½æŒ‡æ ‡
    build_time_seconds FLOAT,
    search_latency_ms FLOAT,
    recall_at_10 FLOAT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_used_at TIMESTAMP
);

-- å¢é‡æ›´æ–°æ—¥å¿—
CREATE TABLE vector_updates (
    id BIGSERIAL PRIMARY KEY,
    photo_id BIGINT REFERENCES photos(id),
    
    operation TEXT NOT NULL,  -- 'insert', 'update', 'delete'
    old_vector_id BIGINT,
    new_vector_id BIGINT,
    
    -- æ‰¹æ¬¡ä¿¡æ¯
    batch_id UUID,
    batch_size INT,
    
    -- çŠ¶æ€
    status TEXT DEFAULT 'pending',  -- pending, processing, completed, failed
    error_message TEXT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP
);
```

## ğŸ”„ å¢é‡æ›´æ–°æœºåˆ¶

### 1. å®æ—¶æ›´æ–°ç­–ç•¥ï¼ˆpgvectoråŸç”Ÿï¼‰
```python
# services/vector_updater.py
from typing import List, Tuple, Optional
import numpy as np
from datetime import datetime
import asyncpg

class PgVectorUpdater:
    """PostgreSQLå‘é‡æ›´æ–°æœåŠ¡"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db = db_pool
        
    async def add_vector(self, photo_id: int, embedding: np.ndarray, 
                        model_name: str = "clip-vit-base") -> int:
        """æ·»åŠ æ–°å‘é‡"""
        async with self.db.acquire() as conn:
            # pgvectorè‡ªåŠ¨å¤„ç†ç´¢å¼•æ›´æ–°
            vector_id = await conn.fetchval("""
                INSERT INTO photo_embeddings 
                (photo_id, embedding, embedding_model, embedding_version, embedding_dimension)
                VALUES ($1, $2, $3, $4, $5)
                RETURNING id
            """, photo_id, embedding.tolist(), model_name, "v1.0.0", len(embedding))
            
            # è®°å½•æ“ä½œæ—¥å¿—
            await self.log_update(conn, 'insert', photo_id, vector_id)
            
        return vector_id
    
    async def update_vector(self, photo_id: int, new_embedding: np.ndarray) -> int:
        """æ›´æ–°ç°æœ‰å‘é‡ï¼ˆåŸå­æ“ä½œï¼‰"""
        async with self.db.acquire() as conn:
            async with conn.transaction():
                # è½¯åˆ é™¤æ—§å‘é‡
                await conn.execute("""
                    UPDATE photo_embeddings 
                    SET is_active = FALSE, updated_at = CURRENT_TIMESTAMP
                    WHERE photo_id = $1 AND is_active = TRUE
                """, photo_id)
                
                # æ’å…¥æ–°å‘é‡
                new_id = await conn.fetchval("""
                    INSERT INTO photo_embeddings 
                    (photo_id, embedding, embedding_model, embedding_version, embedding_dimension)
                    VALUES ($1, $2, $3, $4, $5)
                    RETURNING id
                """, photo_id, new_embedding.tolist(), "clip-vit-base", "v1.0.0", len(new_embedding))
                
                # è®°å½•æ›´æ–°
                await self.log_update(conn, 'update', photo_id, new_id)
                
        return new_id
    
    async def batch_add_vectors(self, vectors: List[Tuple[int, np.ndarray]]) -> List[int]:
        """æ‰¹é‡æ·»åŠ å‘é‡ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰"""
        async with self.db.acquire() as conn:
            # ä½¿ç”¨COPYå‘½ä»¤æ‰¹é‡æ’å…¥ï¼Œæ€§èƒ½æœ€ä¼˜
            result = await conn.copy_records_to_table(
                'photo_embeddings',
                records=[(pid, emb.tolist(), "clip-vit-base", "v1.0.0", len(emb)) 
                        for pid, emb in vectors],
                columns=['photo_id', 'embedding', 'embedding_model', 
                        'embedding_version', 'embedding_dimension']
            )
            
            return result
    
    async def search_similar(self, query_embedding: np.ndarray, 
                            limit: int = 10) -> List[dict]:
        """ç›¸ä¼¼å‘é‡æœç´¢"""
        async with self.db.acquire() as conn:
            # pgvectoråŸç”Ÿç›¸ä¼¼åº¦æœç´¢
            results = await conn.fetch("""
                SELECT 
                    p.photo_id,
                    p.embedding <=> $1 as distance,
                    ph.path,
                    ph.category
                FROM photo_embeddings p
                JOIN photos ph ON p.photo_id = ph.id
                WHERE p.is_active = TRUE
                ORDER BY p.embedding <=> $1
                LIMIT $2
            """, query_embedding.tolist(), limit)
            
            return [dict(r) for r in results]
    
    async def optimize_index(self):
        """ä¼˜åŒ–å‘é‡ç´¢å¼•"""
        async with self.db.acquire() as conn:
            # REINDEXä¼˜åŒ–ç´¢å¼•æ€§èƒ½
            await conn.execute("REINDEX INDEX CONCURRENTLY photo_embeddings_hnsw_idx")
            
            # æ¸…ç†è½¯åˆ é™¤çš„å‘é‡
            await conn.execute("""
                DELETE FROM photo_embeddings 
                WHERE is_active = FALSE 
                AND updated_at < CURRENT_TIMESTAMP - INTERVAL '7 days'
            """)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            await conn.execute("ANALYZE photo_embeddings")
```

### 2. æ‰¹é‡æ›´æ–°ä¼˜åŒ–
```python
# services/batch_updater.py
class BatchVectorUpdater:
    """æ‰¹é‡å‘é‡æ›´æ–°ä¼˜åŒ–"""
    
    async def batch_update(self, updates: List[Tuple[int, np.ndarray]]):
        """æ‰¹é‡æ›´æ–°å‘é‡"""
        batch_id = uuid.uuid4()
        
        # 1. æ‰¹é‡éªŒè¯
        valid_updates = await self.validate_batch(updates)
        
        # 2. äº‹åŠ¡å¤„ç†
        async with self.db.transaction():
            # æ‰¹é‡å†™å…¥PostgreSQL
            vector_ids = await self.bulk_insert_postgres(valid_updates, batch_id)
            
            # æ‰¹é‡æ›´æ–°Faiss
            vectors = np.vstack([u[1] for u in valid_updates])
            self.add_batch_to_faiss(vectors)
            
            # è®°å½•æ‰¹æ¬¡æ—¥å¿—
            await self.log_batch_update(batch_id, len(valid_updates))
        
        # 3. å¼‚æ­¥åå¤„ç†
        asyncio.create_task(self.post_process_batch(batch_id))
        
        return {'batch_id': batch_id, 'processed': len(valid_updates)}
    
    async def post_process_batch(self, batch_id: str):
        """æ‰¹æ¬¡åå¤„ç†"""
        # æ›´æ–°ç´¢å¼•ç»Ÿè®¡
        await self.update_index_statistics()
        
        # è§¦å‘ç´¢å¼•ä¼˜åŒ–
        if await self.should_optimize():
            await self.optimize_index()
        
        # æ¸…ç†æ—§å‘é‡
        await self.cleanup_old_vectors(batch_id)
```

### 3. ç‰ˆæœ¬è¿ç§»ç­–ç•¥
```python
# services/version_migration.py
class VectorVersionMigrator:
    """å‘é‡ç‰ˆæœ¬è¿ç§»æœåŠ¡"""
    
    async def migrate_to_new_model(self, 
                                   old_model: str, 
                                   new_model: str,
                                   batch_size: int = 1000):
        """è¿ç§»åˆ°æ–°æ¨¡å‹ç‰ˆæœ¬"""
        
        # 1. åˆ›å»ºæ–°ç´¢å¼•
        new_index = await self.create_new_index(new_model)
        
        # 2. åˆ†æ‰¹è¿ç§»
        total_photos = await self.get_photo_count()
        
        for offset in range(0, total_photos, batch_size):
            # è·å–æ‰¹æ¬¡ç…§ç‰‡
            photos = await self.get_photos_batch(offset, batch_size)
            
            # ç”Ÿæˆæ–°å‘é‡
            new_vectors = await self.generate_embeddings(photos, new_model)
            
            # å†™å…¥æ–°ç´¢å¼•
            await self.add_to_new_index(new_index, new_vectors)
            
            # æ›´æ–°è¿›åº¦
            progress = (offset + batch_size) / total_photos
            await self.update_migration_progress(new_model, progress)
        
        # 3. åˆ‡æ¢ç´¢å¼•
        await self.atomic_switch_index(old_model, new_model)
        
        # 4. éªŒè¯è¿ç§»
        await self.validate_migration(old_model, new_model)
```

## ğŸ· æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

### 1. æ¨¡å‹æ³¨å†Œè¡¨
```sql
-- æ¨¡å‹ç‰ˆæœ¬æ³¨å†Œè¡¨
CREATE TABLE model_registry (
    id SERIAL PRIMARY KEY,
    model_name TEXT NOT NULL,  -- 'clip-detector', 'rtmdet', 'few-shot-v1'
    model_type TEXT NOT NULL,  -- 'detection', 'embedding', 'classification'
    
    -- ç‰ˆæœ¬ä¿¡æ¯
    version TEXT NOT NULL,  -- è¯­ä¹‰ç‰ˆæœ¬å· '1.2.3'
    version_tag TEXT,  -- 'stable', 'beta', 'deprecated'
    
    -- æ¨¡å‹æ–‡ä»¶
    model_path TEXT NOT NULL,
    config_path TEXT,
    checkpoint_path TEXT,
    file_size_mb FLOAT,
    
    -- æ€§èƒ½æŒ‡æ ‡
    metrics JSONB,  -- {"accuracy": 0.92, "f1": 0.89, "latency_ms": 45}
    benchmark_results JSONB,
    
    -- ä¾èµ–å…³ç³»
    parent_version TEXT,  -- åŸºäºå“ªä¸ªç‰ˆæœ¬
    dependencies JSONB,  -- {"transformers": "4.30.0"}
    
    -- éƒ¨ç½²ä¿¡æ¯
    deployment_status TEXT DEFAULT 'testing',  -- testing, staging, production
    deployed_at TIMESTAMP,
    deprecated_at TIMESTAMP,
    
    -- å…ƒæ•°æ®
    description TEXT,
    changelog TEXT,
    created_by TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(model_name, version)
);

-- æ¨¡å‹éƒ¨ç½²å†å²
CREATE TABLE model_deployments (
    id SERIAL PRIMARY KEY,
    model_id INT REFERENCES model_registry(id),
    
    environment TEXT NOT NULL,  -- 'dev', 'staging', 'prod'
    deployed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    deployed_by TEXT,
    
    -- éƒ¨ç½²é…ç½®
    config JSONB,
    resources JSONB,  -- {"cpu": 4, "memory": "8GB", "gpu": "V100"}
    
    -- å›æ»šä¿¡æ¯
    rollback_from INT REFERENCES model_deployments(id),
    rollback_reason TEXT,
    rollback_at TIMESTAMP
);

-- A/Bæµ‹è¯•é…ç½®
CREATE TABLE model_ab_tests (
    id SERIAL PRIMARY KEY,
    test_name TEXT UNIQUE NOT NULL,
    
    -- æµ‹è¯•æ¨¡å‹
    model_a_id INT REFERENCES model_registry(id),
    model_b_id INT REFERENCES model_registry(id),
    
    -- æµé‡åˆ†é…
    traffic_split FLOAT DEFAULT 0.5,  -- Aæ¨¡å‹æµé‡æ¯”ä¾‹
    
    -- æµ‹è¯•é…ç½®
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    
    -- æµ‹è¯•ç»“æœ
    metrics_a JSONB,
    metrics_b JSONB,
    winner TEXT,  -- 'model_a', 'model_b', 'no_difference'
    
    status TEXT DEFAULT 'running'  -- 'planning', 'running', 'completed'
);
```

### 2. æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
```python
# services/model_versioning.py
from semantic_version import Version
import hashlib

class ModelVersionManager:
    """æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self):
        self.registry = ModelRegistry()
        self.storage = ModelStorage()
    
    async def register_model(self, 
                            model_name: str,
                            model_path: str,
                            metrics: dict,
                            auto_version: bool = True):
        """æ³¨å†Œæ–°æ¨¡å‹ç‰ˆæœ¬"""
        
        # 1. è®¡ç®—æ¨¡å‹å“ˆå¸Œ
        model_hash = self.calculate_model_hash(model_path)
        
        # 2. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if await self.model_exists(model_hash):
            raise ModelAlreadyExistsError(f"Model {model_hash} already registered")
        
        # 3. è‡ªåŠ¨ç‰ˆæœ¬å·
        if auto_version:
            version = await self.get_next_version(model_name, metrics)
        
        # 4. ä¿å­˜æ¨¡å‹æ–‡ä»¶
        stored_path = await self.storage.save_model(
            model_path, 
            f"{model_name}/{version}"
        )
        
        # 5. æ³¨å†Œåˆ°æ•°æ®åº“
        model_id = await self.registry.register(
            model_name=model_name,
            version=version,
            path=stored_path,
            metrics=metrics
        )
        
        # 6. è¿è¡ŒéªŒè¯æµ‹è¯•
        await self.validate_model(model_id)
        
        return model_id
    
    async def get_next_version(self, model_name: str, metrics: dict) -> str:
        """è‡ªåŠ¨ç”Ÿæˆç‰ˆæœ¬å·"""
        current = await self.get_latest_version(model_name)
        
        if not current:
            return "1.0.0"
        
        version = Version(current)
        
        # æ ¹æ®æ€§èƒ½å†³å®šç‰ˆæœ¬å·
        current_metrics = await self.get_model_metrics(model_name, current)
        
        if metrics.get('accuracy', 0) > current_metrics.get('accuracy', 0) * 1.1:
            # æ€§èƒ½æå‡>10%ï¼Œä¸»ç‰ˆæœ¬å·+1
            return str(version.next_major())
        elif metrics.get('accuracy', 0) > current_metrics.get('accuracy', 0):
            # æ€§èƒ½æå‡ï¼Œæ¬¡ç‰ˆæœ¬å·+1
            return str(version.next_minor())
        else:
            # è¡¥ä¸ç‰ˆæœ¬
            return str(version.next_patch())
    
    async def deploy_model(self, 
                          model_id: int, 
                          environment: str,
                          strategy: str = 'blue_green'):
        """éƒ¨ç½²æ¨¡å‹"""
        
        if strategy == 'blue_green':
            return await self.blue_green_deploy(model_id, environment)
        elif strategy == 'canary':
            return await self.canary_deploy(model_id, environment)
        elif strategy == 'ab_test':
            return await self.ab_test_deploy(model_id, environment)
        else:
            return await self.direct_deploy(model_id, environment)
```

### 3. æ¨¡å‹çƒ­æ›´æ–°
```python
# services/model_hot_reload.py
class ModelHotReloader:
    """æ¨¡å‹çƒ­æ›´æ–°æœåŠ¡"""
    
    def __init__(self):
        self.active_models = {}
        self.model_locks = {}
    
    async def hot_reload(self, model_name: str, new_version: str):
        """çƒ­æ›´æ–°æ¨¡å‹"""
        
        # 1. é¢„åŠ è½½æ–°æ¨¡å‹
        new_model = await self.preload_model(model_name, new_version)
        
        # 2. è·å–å†™é”
        async with self.get_model_lock(model_name, mode='write'):
            # 3. å¤‡ä»½å½“å‰æ¨¡å‹
            old_model = self.active_models.get(model_name)
            
            # 4. åŸå­åˆ‡æ¢
            self.active_models[model_name] = new_model
            
            # 5. éªŒè¯æ–°æ¨¡å‹
            try:
                await self.validate_loaded_model(model_name)
            except ValidationError as e:
                # å›æ»š
                self.active_models[model_name] = old_model
                raise ModelUpdateError(f"Validation failed: {e}")
            
            # 6. æ¸…ç†æ—§æ¨¡å‹
            if old_model:
                await self.cleanup_model(old_model)
        
        # 7. æ›´æ–°è·¯ç”±
        await self.update_model_routing(model_name, new_version)
        
        return {"status": "success", "model": model_name, "version": new_version}
    
    async def validate_loaded_model(self, model_name: str):
        """éªŒè¯åŠ è½½çš„æ¨¡å‹"""
        model = self.active_models[model_name]
        
        # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        test_cases = await self.get_test_cases(model_name)
        for test in test_cases:
            result = model.predict(test.input)
            assert result.shape == test.expected_shape
            assert result.dtype == test.expected_dtype
```

## ğŸ”„ å¤‡ä»½ä¸æ¢å¤æœºåˆ¶

### 1. ç®€åŒ–çš„å¤‡ä»½ç­–ç•¥ï¼ˆå•ä¸€æ•°æ®æºä¼˜åŠ¿ï¼‰
```python
# services/backup_service.py
class PgVectorBackupService:
    """PostgreSQLå‘é‡å¤‡ä»½æœåŠ¡"""
    
    async def backup_vectors(self, backup_name: str):
        """å¤‡ä»½å‘é‡æ•°æ®ï¼ˆåˆ©ç”¨PostgreSQLåŸç”Ÿå¤‡ä»½ï¼‰"""
        
        # 1. ä½¿ç”¨pg_dumpå¤‡ä»½å‘é‡è¡¨
        backup_cmd = f"""
        pg_dump -h localhost -U user -d vibe_photos \
                -t photo_embeddings -t vector_updates \
                -f backups/{backup_name}_vectors.sql
        """
        await self.execute_backup(backup_cmd)
        
        # 2. åˆ›å»ºå¤‡ä»½å…ƒæ•°æ®
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'total_vectors': await self.get_vector_count(),
            'backup_file': f"{backup_name}_vectors.sql",
            'index_type': 'hnsw',
            'model_versions': await self.get_active_model_versions()
        }
        
        # 3. å¯é€‰ï¼šä¸Šä¼ åˆ°S3
        if self.use_s3_backup:
            await self.upload_to_s3(backup_name, metadata)
        
        return metadata
    
    async def restore_from_backup(self, backup_name: str):
        """æ¢å¤å‘é‡æ•°æ®"""
        
        # 1. æ¢å¤PostgreSQLæ•°æ®
        restore_cmd = f"""
        psql -h localhost -U user -d vibe_photos \
             -f backups/{backup_name}_vectors.sql
        """
        await self.execute_restore(restore_cmd)
        
        # 2. é‡å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        await self.rebuild_indices()
        
        # 3. éªŒè¯æ•°æ®å®Œæ•´æ€§
        await self.validate_restoration()
    
    async def incremental_backup(self, since: datetime):
        """å¢é‡å¤‡ä»½ï¼ˆä»…å¤‡ä»½å˜æ›´ï¼‰"""
        async with self.db.acquire() as conn:
            changes = await conn.fetch("""
                SELECT * FROM photo_embeddings 
                WHERE created_at > $1 OR updated_at > $1
            """, since)
            
            # å¯¼å‡ºä¸ºJSONæˆ–å…¶ä»–æ ¼å¼
            await self.export_changes(changes)
```

## ğŸš€ å¯é€‰æ€§èƒ½ä¼˜åŒ–ï¼šä½•æ—¶å¼•å…¥Faiss

### è§¦å‘æ¡ä»¶
```yaml
consider_faiss_when:
  vector_count: > 1,000,000  # è¶…è¿‡ç™¾ä¸‡å‘é‡
  query_latency: > 200ms     # æŸ¥è¯¢å»¶è¿Ÿè¿‡é«˜
  concurrent_users: > 1000    # é«˜å¹¶å‘åœºæ™¯
  special_requirements:
    - éœ€è¦GPUåŠ é€Ÿ
    - éœ€è¦ç‰¹æ®Šç´¢å¼•ç±»å‹ï¼ˆPQã€LSHç­‰ï¼‰
    - éœ€è¦æè‡´çš„æ‰¹é‡æŸ¥è¯¢æ€§èƒ½

current_status:
  our_scale: 30,000 vectors  # è¿œä½äºé˜ˆå€¼
  expected_latency: < 20ms   # pgvectorè¶³å¤Ÿå¿«
  conclusion: "ä¸éœ€è¦Faiss"  # âœ…
```

### æœªæ¥æ‰©å±•è·¯å¾„ï¼ˆå¦‚éœ€è¦ï¼‰
```python
# services/hybrid_search.py (ä»…å½“è§„æ¨¡è¶…è¿‡ç™¾ä¸‡æ—¶)
class HybridVectorSearch:
    """æ··åˆæœç´¢ç­–ç•¥ï¼špgvector + Faissç¼“å­˜"""
    
    def __init__(self):
        self.pg_searcher = PgVectorSearcher()
        self.faiss_cache = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
    async def search(self, query_vector, limit=10):
        # é»˜è®¤ä½¿ç”¨pgvector
        if self.should_use_faiss():
            # ä»…å¯¹çƒ­é—¨æŸ¥è¯¢ä½¿ç”¨Faissç¼“å­˜
            return await self.faiss_cached_search(query_vector, limit)
        else:
            # å¸¸è§„æŸ¥è¯¢ç›´æ¥ç”¨pgvector
            return await self.pg_searcher.search(query_vector, limit)
    
    def should_use_faiss(self):
        # åŠ¨æ€åˆ¤æ–­æ˜¯å¦éœ€è¦Faiss
        return (
            self.total_vectors > 1_000_000 or
            self.avg_query_latency > 200  # ms
        )
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### ç›‘æ§æŒ‡æ ‡
```python
# monitoring/vector_metrics.py
from prometheus_client import Histogram, Counter, Gauge

# æ€§èƒ½æŒ‡æ ‡
vector_search_latency = Histogram(
    'vector_search_latency_seconds',
    'Vector search latency',
    ['index_type', 'query_type']
)

vector_update_latency = Histogram(
    'vector_update_latency_seconds',
    'Vector update latency',
    ['operation']
)

# æ•°æ®æŒ‡æ ‡
total_vectors = Gauge(
    'total_vectors_count',
    'Total number of vectors',
    ['model', 'version']
)

index_size_bytes = Gauge(
    'index_size_bytes',
    'Index file size in bytes',
    ['index_name']
)

# ä¸€è‡´æ€§æŒ‡æ ‡
sync_lag_seconds = Gauge(
    'vector_sync_lag_seconds',
    'Sync lag between PostgreSQL and Faiss'
)

inconsistency_count = Counter(
    'vector_inconsistency_total',
    'Total inconsistencies detected',
    ['type']
)
```

## ğŸ“‹ å®æ–½æ£€æŸ¥æ¸…å•

### å‘é‡æ•°æ®åº“ï¼ˆpgvectorä¸ºä¸»ï¼‰
- [ ] å®‰è£…PostgreSQL 14+
- [ ] å®‰è£…pgvectoræ‰©å±•
- [ ] åˆ›å»ºå‘é‡è¡¨å’ŒHNSWç´¢å¼•
- [ ] å®ç°å‘é‡CRUDæ“ä½œ
- [ ] é…ç½®æ‰¹é‡æ’å…¥ä¼˜åŒ–
- [ ] å®ç°æ··åˆæŸ¥è¯¢ï¼ˆSQL + å‘é‡ï¼‰
- [ ] è®¾ç½®è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
- [ ] é…ç½®PostgreSQLå¤‡ä»½ç­–ç•¥
- [ ] æ·»åŠ æŸ¥è¯¢æ€§èƒ½ç›‘æ§
- [ ] å‡†å¤‡Faissæ‰©å±•æ–¹æ¡ˆï¼ˆé¢„ç•™ï¼‰

### æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- [ ] åˆ›å»ºæ¨¡å‹æ³¨å†Œè¡¨
- [ ] å®ç°ç‰ˆæœ¬æ§åˆ¶é€»è¾‘
- [ ] é…ç½®çƒ­æ›´æ–°æœºåˆ¶
- [ ] è®¾ç½®A/Bæµ‹è¯•æ¡†æ¶
- [ ] å®ç°è“ç»¿éƒ¨ç½²
- [ ] é…ç½®å›æ»šç­–ç•¥
- [ ] æ·»åŠ æ¨¡å‹éªŒè¯æµç¨‹
- [ ] è®¾ç½®æ€§èƒ½åŸºå‡†æµ‹è¯•

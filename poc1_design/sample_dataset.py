#!/usr/bin/env python3
"""
æ•°æ®é›†é‡‡æ ·è„šæœ¬
ç”¨äºä»30,000+å¼ ç…§ç‰‡ä¸­æ™ºèƒ½é‡‡æ ·ï¼Œç”ŸæˆPoC1æµ‹è¯•é›†
"""

import os
import random
import shutil
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Tuple

class DatasetSampler:
    """æ•°æ®é›†é‡‡æ ·å™¨"""
    
    def __init__(self, source_dir: str, target_dir: str):
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir)
        self.metadata = {
            'source_path': str(source_dir),
            'target_path': str(target_dir),
            'created_at': datetime.now().isoformat(),
            'samples': []
        }
    
    def analyze_dataset(self) -> Dict:
        """åˆ†ææ•°æ®é›†ç»“æ„"""
        print("ğŸ“Š åˆ†ææ•°æ®é›†...")
        
        stats = {
            'total_dirs': 0,
            'total_files': 0,
            'by_year': defaultdict(int),
            'by_location': defaultdict(int),
            'file_types': defaultdict(int),
            'size_distribution': []
        }
        
        for dir_path in self.source_dir.iterdir():
            if not dir_path.is_dir():
                continue
            
            stats['total_dirs'] += 1
            
            # è§£æç›®å½•åï¼ˆå¦‚ï¼šBeijing, October 29, 2025ï¼‰
            dir_name = dir_path.name
            year = self._extract_year(dir_name)
            location = self._extract_location(dir_name)
            
            if year:
                stats['by_year'][year] += 1
            if location:
                stats['by_location'][location] += 1
            
            # ç»Ÿè®¡æ–‡ä»¶
            for file_path in dir_path.iterdir():
                if file_path.is_file():
                    stats['total_files'] += 1
                    ext = file_path.suffix.lower()
                    stats['file_types'][ext] += 1
                    
                    # è®°å½•æ–‡ä»¶å¤§å°
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    stats['size_distribution'].append(size_mb)
        
        return stats
    
    def _extract_year(self, dir_name: str) -> str:
        """ä»ç›®å½•åæå–å¹´ä»½"""
        import re
        match = re.search(r'20\d{2}', dir_name)
        return match.group() if match else None
    
    def _extract_location(self, dir_name: str) -> str:
        """ä»ç›®å½•åæå–åœ°ç‚¹"""
        # æ ¼å¼ï¼šåœ°ç‚¹, æ—¥æœŸ æˆ– åªæœ‰æ—¥æœŸ
        parts = dir_name.split(',')
        if len(parts) > 1:
            return parts[0].strip()
        return None
    
    def stratified_sample(self, 
                          total_samples: int = 1000,
                          strategy: str = 'balanced') -> List[Path]:
        """
        åˆ†å±‚é‡‡æ ·
        
        ç­–ç•¥ï¼š
        - balanced: æŒ‰å¹´ä»½å‡åŒ€é‡‡æ ·
        - recent: åå‘æœ€è¿‘çš„ç…§ç‰‡
        - random: å®Œå…¨éšæœº
        """
        print(f"ğŸ² æ‰§è¡Œ{strategy}é‡‡æ ·ï¼Œç›®æ ‡ï¼š{total_samples}å¼ ")
        
        all_images = []
        year_groups = defaultdict(list)
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡å¹¶æŒ‰å¹´ä»½åˆ†ç»„
        for dir_path in self.source_dir.iterdir():
            if not dir_path.is_dir():
                continue
            
            year = self._extract_year(dir_path.name) or "unknown"
            
            for file_path in dir_path.iterdir():
                if file_path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.heic']:
                    all_images.append(file_path)
                    year_groups[year].append(file_path)
        
        # æ ¹æ®ç­–ç•¥é‡‡æ ·
        if strategy == 'balanced':
            # æ¯å¹´å‡åŒ€é‡‡æ ·
            samples = []
            years = sorted(year_groups.keys())
            samples_per_year = total_samples // len(years)
            
            for year in years:
                year_images = year_groups[year]
                n = min(samples_per_year, len(year_images))
                samples.extend(random.sample(year_images, n))
            
            # è¡¥å……ä¸è¶³çš„æ ·æœ¬
            if len(samples) < total_samples:
                remaining = total_samples - len(samples)
                pool = [img for img in all_images if img not in samples]
                samples.extend(random.sample(pool, min(remaining, len(pool))))
        
        elif strategy == 'recent':
            # åå‘æœ€è¿‘çš„ç…§ç‰‡ï¼ˆ70%æœ€è¿‘3å¹´ï¼Œ30%å…¶ä»–ï¼‰
            recent_years = ['2023', '2024', '2025']
            recent_images = []
            older_images = []
            
            for year, images in year_groups.items():
                if year in recent_years:
                    recent_images.extend(images)
                else:
                    older_images.extend(images)
            
            recent_count = int(total_samples * 0.7)
            older_count = total_samples - recent_count
            
            samples = []
            if recent_images:
                samples.extend(random.sample(recent_images, 
                              min(recent_count, len(recent_images))))
            if older_images:
                samples.extend(random.sample(older_images, 
                              min(older_count, len(older_images))))
        
        else:  # random
            # å®Œå…¨éšæœº
            samples = random.sample(all_images, min(total_samples, len(all_images)))
        
        return samples[:total_samples]
    
    def create_sample_dataset(self, 
                             samples: List[Path],
                             preserve_structure: bool = True):
        """åˆ›å»ºé‡‡æ ·æ•°æ®é›†"""
        print(f"ğŸ“ åˆ›å»ºé‡‡æ ·æ•°æ®é›†ï¼š{len(samples)}ä¸ªæ–‡ä»¶")
        
        self.target_dir.mkdir(parents=True, exist_ok=True)
        
        for i, source_file in enumerate(samples, 1):
            if preserve_structure:
                # ä¿æŒåŸç›®å½•ç»“æ„
                rel_dir = source_file.parent.name
                target_subdir = self.target_dir / rel_dir
                target_subdir.mkdir(exist_ok=True)
                target_file = target_subdir / source_file.name
            else:
                # æ‰å¹³åŒ–ç»“æ„
                target_file = self.target_dir / f"{i:04d}_{source_file.name}"
            
            # å¤åˆ¶æ–‡ä»¶ï¼ˆæˆ–åˆ›å»ºç¬¦å·é“¾æ¥èŠ‚çœç©ºé—´ï¼‰
            if not target_file.exists():
                # ä½¿ç”¨ç¬¦å·é“¾æ¥èŠ‚çœç©ºé—´
                target_file.symlink_to(source_file)
                # æˆ–è€…å¤åˆ¶ï¼šshutil.copy2(source_file, target_file)
            
            # è®°å½•å…ƒæ•°æ®
            self.metadata['samples'].append({
                'index': i,
                'source': str(source_file),
                'target': str(target_file),
                'size_mb': source_file.stat().st_size / (1024 * 1024)
            })
            
            if i % 100 == 0:
                print(f"  å·²å¤„ç† {i}/{len(samples)} æ–‡ä»¶...")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = self.target_dir / 'dataset_metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é‡‡æ ·å®Œæˆï¼å…ƒæ•°æ®ä¿å­˜è‡³ï¼š{metadata_file}")
    
    def create_test_sets(self):
        """åˆ›å»ºå¤šä¸ªæµ‹è¯•é›†"""
        print("ğŸ”¬ åˆ›å»ºæµ‹è¯•é›†...")
        
        # Phase 1: å°è§„æ¨¡éªŒè¯é›†ï¼ˆ1000å¼ ï¼‰
        print("\n--- Phase 1: éªŒè¯é›† ---")
        validation_samples = self.stratified_sample(1000, 'balanced')
        validation_dir = self.target_dir / 'phase1_validation'
        sampler = DatasetSampler(self.source_dir, validation_dir)
        sampler.create_sample_dataset(validation_samples)
        
        # Phase 2: æ€§èƒ½æµ‹è¯•é›†ï¼ˆ5000å¼ ï¼‰
        print("\n--- Phase 2: æ€§èƒ½æµ‹è¯•é›† ---")
        performance_samples = self.stratified_sample(5000, 'recent')
        performance_dir = self.target_dir / 'phase2_performance'
        sampler = DatasetSampler(self.source_dir, performance_dir)
        sampler.create_sample_dataset(performance_samples)
        
        # Phase 3: å‡†å¤‡å…¨é‡å¤„ç†
        print("\n--- Phase 3: å…¨é‡æ•°æ®é›† ---")
        print(f"å…¨é‡æ•°æ®é›†è·¯å¾„ï¼š{self.source_dir}")
        print(f"åŒ…å« 30,000+ å¼ ç…§ç‰‡ï¼Œ400GB æ•°æ®")
        print("å»ºè®®ä½¿ç”¨å¢é‡å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
    
    def generate_report(self):
        """ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š"""
        stats = self.analyze_dataset()
        
        report = f"""
# æ•°æ®é›†åˆ†ææŠ¥å‘Š

## æ¦‚è§ˆ
- æ€»ç›®å½•æ•°ï¼š{stats['total_dirs']:,}
- æ€»æ–‡ä»¶æ•°ï¼š{stats['total_files']:,}

## å¹´ä»½åˆ†å¸ƒ
"""
        for year in sorted(stats['by_year'].keys()):
            count = stats['by_year'][year]
            report += f"- {year}: {count} ä¸ªç›®å½•\n"
        
        report += "\n## åœ°ç‚¹åˆ†å¸ƒï¼ˆTop 10ï¼‰\n"
        locations = sorted(stats['by_location'].items(), key=lambda x: x[1], reverse=True)[:10]
        for location, count in locations:
            report += f"- {location}: {count} ä¸ªç›®å½•\n"
        
        report += "\n## æ–‡ä»¶ç±»å‹\n"
        for ext, count in stats['file_types'].items():
            report += f"- {ext}: {count:,} ä¸ªæ–‡ä»¶\n"
        
        if stats['size_distribution']:
            avg_size = sum(stats['size_distribution']) / len(stats['size_distribution'])
            max_size = max(stats['size_distribution'])
            min_size = min(stats['size_distribution'])
            
            report += f"""
## æ–‡ä»¶å¤§å°
- å¹³å‡: {avg_size:.1f} MB
- æœ€å¤§: {max_size:.1f} MB
- æœ€å°: {min_size:.1f} MB
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®é›†é‡‡æ ·å·¥å…·')
    parser.add_argument('--source', default='/Users/jasl/Workspaces/exported_photos',
                       help='æºæ•°æ®ç›®å½•')
    parser.add_argument('--target', default='./test_datasets',
                       help='ç›®æ ‡ç›®å½•')
    parser.add_argument('--samples', type=int, default=1000,
                       help='é‡‡æ ·æ•°é‡')
    parser.add_argument('--strategy', choices=['balanced', 'recent', 'random'],
                       default='balanced', help='é‡‡æ ·ç­–ç•¥')
    parser.add_argument('--analyze-only', action='store_true',
                       help='ä»…åˆ†ææ•°æ®é›†')
    
    args = parser.parse_args()
    
    sampler = DatasetSampler(args.source, args.target)
    
    if args.analyze_only:
        # ä»…åˆ†æ
        report = sampler.generate_report()
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(args.target) / 'dataset_analysis.md'
        report_file.parent.mkdir(exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_file}")
    else:
        # æ‰§è¡Œé‡‡æ ·
        samples = sampler.stratified_sample(args.samples, args.strategy)
        sampler.create_sample_dataset(samples)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = sampler.generate_report()
        report_file = Path(args.target) / 'sampling_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

if __name__ == "__main__":
    main()
